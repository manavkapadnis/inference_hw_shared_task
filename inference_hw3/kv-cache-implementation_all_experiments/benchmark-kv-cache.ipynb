{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8fb6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import json, os\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0e8713",
   "metadata": {},
   "source": [
    "## Benchmarking with vs without KV caching\n",
    "\n",
    "1. Use [`meta-llama/Llama-3.1-8B`](https://huggingface.co/meta-llama/Llama-3.1-8B) and sweep `max_new_tokens`=`min_new_tokens`  by powers of two from 1 to 512, using the same short prompt (\"Once upon a time,\") for all output sequence lengths. Plot time vs output sequence length.\n",
    "2. Now repeat the same with `use_cache=False` in `model.generate()` -- add this plot to the same figure. Make sure to include a legend and descriptive labels/titles. Describe the trends you see in 1-2 sentences -- play around with both log scales and linear scales for a clearer idea of the trends -- **you will only need to include one version in your report**, however.\n",
    "3. Repeat 1-2 with a much longer prompt, read in from `long_prompt.txt`. **For this question, you only need to sweep from 1 to 32 without the KV cache -- with KV cache, you should be able to reach 512 without issues.** In ~2 sentences, (instead of just comparing KV cache vs no KV cache) compare the trends you see with those from your first plot, and provide an explanation for the trends you observe.\n",
    "4. Repeat 1-2 with [`meta-llama/Llama-3.2-1B`](https://huggingface.co/meta-llama/Llama-3.2-1B). Again, in ~2 sentences, compare the trends you see with those from your first plot, and provide an explanation for the trends you observe.\n",
    "5. Repeat 1-2 with [`meta-llama/Llama-2-7b`](https://huggingface.co/meta-llama/Llama-2-7b). Once again, write ~2 sentences to compare the trends you see with those from your first plot, and provide an explanation for the trends you observe.\n",
    "\n",
    "In total, you will include four (4) figures, one (1) explanation of the general kv vs no kv trend, and reflections for three (3) pairwise comparisons in your report. You may need to restart your kernel each time you want to use a new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8194e2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_prompt = \"Once upon a time,\"\n",
    "\n",
    "with open(\"long_prompt.txt\") as f:\n",
    "    long_prompt = f.read().strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0c3742",
   "metadata": {},
   "source": [
    "### One generation example\n",
    "\n",
    "The code below should run without modification in an environment that contains the necessary libraries. A TA used 1 L40S GPU to run these exact cells.\n",
    "\n",
    "You can adapt it to complete the questions above. Again, **you may need to restart your kernel in between loading in different models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1360c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-3.1-8B')\n",
    "model = AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.1-8B', dtype='bfloat16').to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b87c929",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_toks = tokenizer(short_prompt, return_tensors=\"pt\").to('cuda')\n",
    "start = time.time()\n",
    "torch.cuda.synchronize()\n",
    "output = model.generate(**prompt_toks, max_new_tokens=256, use_cache=True) # true by default\n",
    "torch.cuda.synchronize()\n",
    "end = time.time()\n",
    "print(\"Generation of\", output.shape[1]-prompt_toks['input_ids'].shape[1], \"new tokens with KV cache took\", round(end-start), \"seconds.\")\n",
    "print(\"-----\")\n",
    "print(\"Output:\", tokenizer.batch_decode(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf893cce",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84891ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_toks = tokenizer(short_prompt, return_tensors=\"pt\").to('cuda')\n",
    "\n",
    "start = time.time()\n",
    "torch.cuda.synchronize()\n",
    "output = model.generate(**prompt_toks, max_new_tokens=256, use_cache=False) # no KV cache\n",
    "torch.cuda.synchronize()\n",
    "end = time.time()\n",
    "print(\"Generation of\", output.shape[1]-prompt_toks['input_ids'].shape[1], \"new tokens without KV cache took\", round(end-start), \"seconds.\")\n",
    "print(\"-----\")\n",
    "print(\"Output:\", tokenizer.batch_decode(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca085054",
   "metadata": {},
   "source": [
    "### Example plotting code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037829f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "times_with_kv = []\n",
    "times_no_kv = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbeffa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(10), times_with_kv, label=\"with kv cache\") # or use [2**x for x in range(10)] for linear scale\n",
    "plt.plot(range(10), times_no_kv, label=\"no kv cache\")\n",
    "xticks = [f\"2^{x}\" for x in range(10)]\n",
    "plt.xticks(list(range(10)), labels=xticks)\n",
    "plt.legend()\n",
    "# TODO: titles, etc.\n",
    "plt.savefig(\"[YOUR FILEPATH HERE]\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
