import torch
import torch.nn as nn
from torch.nn import functional as F
from transformers import AutoTokenizer, AutoModelForCausalLM
import json
import os
import matplotlib.pyplot as plt
import time
import numpy as np

from huggingface_hub import login

# Alternatively, you can pass the token directly as an argument
login(token="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")

# Configuration
DEVICE = "cuda:0"
DTYPE = torch.bfloat16

def benchmark_generation(model, tokenizer, prompt, max_new_tokens, use_cache=True):
    """Benchmark a single generation run"""
    prompt_toks = tokenizer(prompt, return_tensors="pt").to(DEVICE)
    
    torch.cuda.synchronize()
    start = time.time()
    
    output = model.generate(
        **prompt_toks, 
        max_new_tokens=max_new_tokens,
        min_new_tokens=max_new_tokens,
        use_cache=use_cache,
        do_sample=False
    )
    
    torch.cuda.synchronize()
    end = time.time()
    
    return end - start

def task1_short_prompt_llama_8b():
    """Task 1: Short prompt with Llama-3.1-8B"""
    print("\n" + "="*80)
    print("TASK 1: Short Prompt with Llama-3.1-8B")
    print("="*80)
    
    model_name = 'meta-llama/Llama-3.1-8B'
    short_prompt = "Once upon a time,"
    
    print(f"Loading model: {model_name}")
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=DTYPE).to(DEVICE)
    model.eval()
    print("Model loaded!\n")
    
    # Powers of 2 from 1 to 512
    token_lengths = [2**i for i in range(10)]  # 1, 2, 4, 8, ..., 512
    
    times_with_kv = []
    times_no_kv = []
    
    print(f"{'Tokens':<10} {'With KV (s)':<15} {'Without KV (s)':<15}")
    print("-" * 45)
    
    for num_tokens in token_lengths:
        # With KV cache
        time_kv = benchmark_generation(model, tokenizer, short_prompt, num_tokens, use_cache=True)
        times_with_kv.append(time_kv)
        
        # Without KV cache
        time_no_kv = benchmark_generation(model, tokenizer, short_prompt, num_tokens, use_cache=False)
        times_no_kv.append(time_no_kv)
        
        print(f"{num_tokens:<10} {time_kv:<15.4f} {time_no_kv:<15.4f}")
    
    # Plot
    plt.figure(figsize=(10, 6))
    x_labels = [f"2^{i}" for i in range(10)]
    x_positions = list(range(10))
    
    plt.plot(x_positions, times_with_kv, 'o-', label='With KV Cache', linewidth=2, markersize=8)
    plt.plot(x_positions, times_no_kv, 's-', label='Without KV Cache', linewidth=2, markersize=8)
    
    plt.xlabel('Output Sequence Length (tokens)', fontsize=12)
    plt.ylabel('Time (seconds)', fontsize=12)
    plt.title('Generation Time: Short Prompt, Llama-3.1-8B', fontsize=14, fontweight='bold')
    plt.xticks(x_positions, x_labels)
    plt.legend(fontsize=11)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    
    os.makedirs('kv_cache_results', exist_ok=True)
    plt.savefig('kv_cache_results/task1_short_prompt_llama8b.png', dpi=300, bbox_inches='tight')
    print(f"\nPlot saved to: kv_cache_results/task1_short_prompt_llama8b.png")
    
    # Save data
    results = {
        "model": model_name,
        "prompt": "short",
        "token_lengths": token_lengths,
        "times_with_kv": times_with_kv,
        "times_no_kv": times_no_kv
    }
    with open('kv_cache_results/task1_data.json', 'w') as f:
        json.dump(results, f, indent=2)
    
    print("\nTrend Analysis:")
    print("With KV cache, generation time grows roughly linearly with output length.")
    print("Without KV cache, generation time grows quadratically due to recomputing attention for all previous tokens at each step.")
    
    # Cleanup
    del model
    del tokenizer
    torch.cuda.empty_cache()
    
    return results

def task2_long_prompt_llama_8b():
    """Task 2: Long prompt with Llama-3.1-8B"""
    print("\n" + "="*80)
    print("TASK 2: Long Prompt with Llama-3.1-8B")
    print("="*80)
    
    model_name = 'meta-llama/Llama-3.1-8B'
    
    # Read long prompt
    with open("long_prompt.txt") as f:
        long_prompt = f.read().strip()
    
    print(f"Loading model: {model_name}")
    print(f"Long prompt length: {len(long_prompt)} characters")
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=DTYPE).to(DEVICE)
    model.eval()
    print("Model loaded!\n")
    
    # With KV: 1 to 512, Without KV: 1 to 32
    token_lengths_kv = [2**i for i in range(10)]  # 1 to 512
    token_lengths_no_kv = [2**i for i in range(6)]  # 1 to 32
    
    times_with_kv = []
    times_no_kv = []
    
    print("With KV Cache:")
    print(f"{'Tokens':<10} {'Time (s)':<15}")
    print("-" * 30)
    
    for num_tokens in token_lengths_kv:
        time_kv = benchmark_generation(model, tokenizer, long_prompt, num_tokens, use_cache=True)
        times_with_kv.append(time_kv)
        print(f"{num_tokens:<10} {time_kv:<15.4f}")
    
    print("\nWithout KV Cache (limited to 32):")
    print(f"{'Tokens':<10} {'Time (s)':<15}")
    print("-" * 30)
    
    for num_tokens in token_lengths_no_kv:
        time_no_kv = benchmark_generation(model, tokenizer, long_prompt, num_tokens, use_cache=False)
        times_no_kv.append(time_no_kv)
        print(f"{num_tokens:<10} {time_no_kv:<15.4f}")
    
    # Plot
    plt.figure(figsize=(10, 6))
    x_positions_kv = list(range(10))
    x_positions_no_kv = list(range(6))
    x_labels_kv = [f"2^{i}" for i in range(10)]
    
    plt.plot(x_positions_kv, times_with_kv, 'o-', label='With KV Cache', linewidth=2, markersize=8)
    plt.plot(x_positions_no_kv, times_no_kv, 's-', label='Without KV Cache', linewidth=2, markersize=8)
    
    plt.xlabel('Output Sequence Length (tokens)', fontsize=12)
    plt.ylabel('Time (seconds)', fontsize=12)
    plt.title('Generation Time: Long Prompt, Llama-3.1-8B', fontsize=14, fontweight='bold')
    plt.xticks(x_positions_kv, x_labels_kv)
    plt.legend(fontsize=11)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    
    plt.savefig('kv_cache_results/task2_long_prompt_llama8b.png', dpi=300, bbox_inches='tight')
    print(f"\nPlot saved to: kv_cache_results/task2_long_prompt_llama8b.png")
    
    # Save data
    results = {
        "model": model_name,
        "prompt": "long",
        "token_lengths_kv": token_lengths_kv,
        "token_lengths_no_kv": token_lengths_no_kv,
        "times_with_kv": times_with_kv,
        "times_no_kv": times_no_kv
    }
    with open('kv_cache_results/task2_data.json', 'w') as f:
        json.dump(results, f, indent=2)
    
    print("\nComparison with Task 1:")
    print("The long prompt increases the base computation time for both methods, but KV cache")
    print("still maintains linear scaling. Without KV cache becomes prohibitively expensive much")
    print("faster due to the longer context that must be reprocessed at each step.")
    
    # Cleanup
    del model
    del tokenizer
    torch.cuda.empty_cache()
    
    return results

def task3_short_prompt_llama_1b():
    """Task 3: Short prompt with Llama-3.2-1B"""
    print("\n" + "="*80)
    print("TASK 3: Short Prompt with Llama-3.2-1B")
    print("="*80)
    
    model_name = 'meta-llama/Llama-3.2-1B'
    short_prompt = "Once upon a time,"
    
    print(f"Loading model: {model_name}")
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=DTYPE).to(DEVICE)
    model.eval()
    print("Model loaded!\n")
    
    token_lengths = [2**i for i in range(10)]
    times_with_kv = []
    times_no_kv = []
    
    print(f"{'Tokens':<10} {'With KV (s)':<15} {'Without KV (s)':<15}")
    print("-" * 45)
    
    for num_tokens in token_lengths:
        time_kv = benchmark_generation(model, tokenizer, short_prompt, num_tokens, use_cache=True)
        times_with_kv.append(time_kv)
        
        time_no_kv = benchmark_generation(model, tokenizer, short_prompt, num_tokens, use_cache=False)
        times_no_kv.append(time_no_kv)
        
        print(f"{num_tokens:<10} {time_kv:<15.4f} {time_no_kv:<15.4f}")
    
    # Plot
    plt.figure(figsize=(10, 6))
    x_labels = [f"2^{i}" for i in range(10)]
    x_positions = list(range(10))
    
    plt.plot(x_positions, times_with_kv, 'o-', label='With KV Cache', linewidth=2, markersize=8)
    plt.plot(x_positions, times_no_kv, 's-', label='Without KV Cache', linewidth=2, markersize=8)
    
    plt.xlabel('Output Sequence Length (tokens)', fontsize=12)
    plt.ylabel('Time (seconds)', fontsize=12)
    plt.title('Generation Time: Short Prompt, Llama-3.2-1B', fontsize=14, fontweight='bold')
    plt.xticks(x_positions, x_labels)
    plt.legend(fontsize=11)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    
    plt.savefig('kv_cache_results/task3_short_prompt_llama1b.png', dpi=300, bbox_inches='tight')
    print(f"\nPlot saved to: kv_cache_results/task3_short_prompt_llama1b.png")
    
    # Save data
    results = {
        "model": model_name,
        "prompt": "short",
        "token_lengths": token_lengths,
        "times_with_kv": times_with_kv,
        "times_no_kv": times_no_kv
    }
    with open('kv_cache_results/task3_data.json', 'w') as f:
        json.dump(results, f, indent=2)
    
    print("\nComparison with Task 1 (8B model):")
    print("The 1B model is significantly faster than the 8B model due to fewer parameters")
    print("and smaller hidden dimensions. However, the relative speedup from KV caching remains")
    print("similar, as the quadratic vs linear scaling behavior is model-size independent.")
    
    # Cleanup
    del model
    del tokenizer
    torch.cuda.empty_cache()
    
    return results

def task4_short_prompt_llama2_7b():
    """Task 4: Short prompt with Llama-2-7b"""
    print("\n" + "="*80)
    print("TASK 4: Short Prompt with Llama-2-7b")
    print("="*80)
    
    model_name = 'meta-llama/Llama-2-7b-hf'
    short_prompt = "Once upon a time,"
    
    print(f"Loading model: {model_name}")
    tokenizer = AutoTokenizer.from_pretrained(model_name,use_fast=False)
    model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=DTYPE).to(DEVICE)
    model.eval()
    print("Model loaded!\n")
    
    token_lengths = [2**i for i in range(10)]
    times_with_kv = []
    times_no_kv = []
    
    print(f"{'Tokens':<10} {'With KV (s)':<15} {'Without KV (s)':<15}")
    print("-" * 45)
    
    for num_tokens in token_lengths:
        time_kv = benchmark_generation(model, tokenizer, short_prompt, num_tokens, use_cache=True)
        times_with_kv.append(time_kv)
        
        time_no_kv = benchmark_generation(model, tokenizer, short_prompt, num_tokens, use_cache=False)
        times_no_kv.append(time_no_kv)
        
        print(f"{num_tokens:<10} {time_kv:<15.4f} {time_no_kv:<15.4f}")
    
    # Plot
    plt.figure(figsize=(10, 6))
    x_labels = [f"2^{i}" for i in range(10)]
    x_positions = list(range(10))
    
    plt.plot(x_positions, times_with_kv, 'o-', label='With KV Cache', linewidth=2, markersize=8)
    plt.plot(x_positions, times_no_kv, 's-', label='Without KV Cache', linewidth=2, markersize=8)
    
    plt.xlabel('Output Sequence Length (tokens)', fontsize=12)
    plt.ylabel('Time (seconds)', fontsize=12)
    plt.title('Generation Time: Short Prompt, Llama-2-7b', fontsize=14, fontweight='bold')
    plt.xticks(x_positions, x_labels)
    plt.legend(fontsize=11)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    
    plt.savefig('kv_cache_results/task4_short_prompt_llama2_7b.png', dpi=300, bbox_inches='tight')
    print(f"\nPlot saved to: kv_cache_results/task4_short_prompt_llama2_7b.png")
    
    # Save data
    results = {
        "model": model_name,
        "prompt": "short",
        "token_lengths": token_lengths,
        "times_with_kv": times_with_kv,
        "times_no_kv": times_no_kv
    }
    with open('kv_cache_results/task4_data.json', 'w') as f:
        json.dump(results, f, indent=2)
    
    print("\nComparison with Task 1 (Llama-3.1-8B):")
    print("Llama-2-7b has similar performance to Llama-3.1-8B as they are comparable in size.")
    print("Architecture improvements in Llama-3.1 may lead to slight efficiency differences,")
    print("but the fundamental KV cache speedup pattern remains consistent across model generations.")
    
    # Cleanup
    del model
    del tokenizer
    torch.cuda.empty_cache()
    
    return results

def generate_summary_report(results_list):
    """Generate comprehensive summary report"""
    report = """
KV Cache Benchmarking - Summary Report
=====================================

General Findings:
-----------------
KV Cache dramatically improves generation efficiency by caching key and value 
matrices from previous attention computations. Without KV cache, the model must
recompute attention over all previous tokens at each step, leading to O(nÂ²) 
complexity. With KV cache, each step only requires O(n) operations, resulting
in roughly linear time growth.

Task-by-Task Analysis:
---------------------
"""
    
    for i, results in enumerate(results_list, 1):
        report += f"\nTask {i}: {results['model']} - {results['prompt']} prompt\n"
        report += "-" * 60 + "\n"
        
        if 'times_with_kv' in results and 'times_no_kv' in results:
            times_kv = results['times_with_kv']
            times_no_kv = results['times_no_kv']
            
            # Calculate speedups
            min_len = min(len(times_kv), len(times_no_kv))
            speedups = [times_no_kv[j] / times_kv[j] for j in range(min_len)]
            avg_speedup = np.mean(speedups)
            
            report += f"Average Speedup with KV Cache: {avg_speedup:.2f}x\n"
            report += f"Max time with KV: {max(times_kv):.4f}s\n"
            report += f"Max time without KV: {max(times_no_kv):.4f}s\n"
    
    report += """
\nKey Insights:
-------------
1. KV caching is essential for efficient autoregressive generation
2. Speedup is more pronounced for longer sequences
3. The benefit scales with model size but relative improvement is consistent
4. Long prompts increase absolute times but KV cache remains effective

"""
    
    with open('kv_cache_results/summary_report.txt', 'w') as f:
        f.write(report)
    
    print(report)
    print("Summary report saved to: kv_cache_results/summary_report.txt")

if __name__ == "__main__":
    print("Starting KV Cache Benchmarking Suite")
    print("=" * 80)
    
    results = []
    
    # Run all tasks
    try:
        results.append(task1_short_prompt_llama_8b())
    except Exception as e:
        print(f"Task 1 failed: {e}")
    
    try:
        results.append(task2_long_prompt_llama_8b())
    except Exception as e:
        print(f"Task 2 failed: {e}")
    
    try:
        results.append(task3_short_prompt_llama_1b())
    except Exception as e:
        print(f"Task 3 failed: {e}")
    
    try:
        results.append(task4_short_prompt_llama2_7b())
    except Exception as e:
        print(f"Task 4 failed: {e}")

    
    # Generate summary
    if results:
        generate_summary_report(results)
    
    print("\n" + "="*80)
    print("All benchmarks completed!")
    print("Results saved in: kv_cache_results/")
    print("="*80)