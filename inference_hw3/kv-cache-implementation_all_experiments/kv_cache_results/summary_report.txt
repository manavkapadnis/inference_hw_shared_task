
KV Cache Benchmarking - Summary Report
=====================================

General Findings:
-----------------
KV Cache dramatically improves generation efficiency by caching key and value 
matrices from previous attention computations. Without KV cache, the model must
recompute attention over all previous tokens at each step, leading to O(nÂ²) 
complexity. With KV cache, each step only requires O(n) operations, resulting
in roughly linear time growth.

Task-by-Task Analysis:
---------------------

Task 1: meta-llama/Llama-3.1-8B - short prompt
------------------------------------------------------------
Average Speedup with KV Cache: 0.91x
Max time with KV: 13.2021s
Max time without KV: 18.2423s

Task 2: meta-llama/Llama-3.1-8B - long prompt
------------------------------------------------------------
Average Speedup with KV Cache: 9.18x
Max time with KV: 17.5225s
Max time without KV: 120.0128s

Task 3: meta-llama/Llama-3.2-1B - short prompt
------------------------------------------------------------
Average Speedup with KV Cache: 0.95x
Max time with KV: 7.0568s
Max time without KV: 6.9885s

Task 4: meta-llama/Llama-2-7b-hf - short prompt
------------------------------------------------------------
Average Speedup with KV Cache: 1.01x
Max time with KV: 13.3481s
Max time without KV: 17.1834s


Key Insights:
-------------
1. KV caching is essential for efficient autoregressive generation
2. Speedup is more pronounced for longer sequences
3. The benefit scales with model size but relative improvement is consistent
4. Long prompts increase absolute times but KV cache remains effective

