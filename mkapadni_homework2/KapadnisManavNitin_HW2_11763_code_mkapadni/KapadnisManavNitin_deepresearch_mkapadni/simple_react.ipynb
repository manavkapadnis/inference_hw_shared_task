{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9fd6caf7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys \n",
    "import os\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import pandas as pd\n",
    "import litellm\n",
    "import random\n",
    "import base64\n",
    "import hashlib\n",
    "import json\n",
    "from typing import List, Dict, Optional\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from mcp_agents.tool_interface.base import *\n",
    "from mcp_agents.tool_interface.mcp_tools import *\n",
    "from mcp_agents.client import *\n",
    "from mcp_agents.agent_interface import *\n",
    "from mcp_agents.evaluation_utils.utils import *\n",
    "\n",
    "# !playwright install #to run the crawl4ai tool\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"api key\"\n",
    "os.environ[\"SERPER_API_KEY\"] = \"api key\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c7fca3",
   "metadata": {},
   "source": [
    "### Build a search tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87e3bce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imported from mcp_agents.tool_interface.mcp_tools\n",
    "\n",
    "search_tool = SerperSearchTool(\n",
    "    tool_start_tag=\"<query>\",\n",
    "    tool_end_tag=\"</query>\",\n",
    "    result_start_tag=\"<snippet>\",\n",
    "    result_end_tag=\"</snippet>\",\n",
    "    number_documents_to_search=2,\n",
    "    timeout=60,\n",
    ")\n",
    "\n",
    "client = LLMToolClient(\n",
    "    model_name=\"openai/gpt-4o\",  # Dummy model name\n",
    "    tokenizer_name=\"openai/gpt-4o\",  # Dummy model name\n",
    "    base_url=\"https://api.openai.com/v1\",\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    tools=[search_tool],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6dee221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: 11-664/763: Inference Algorithms for Language Modeling\n",
      "URL: https://www.phontron.com/class/lminference-fall2025/\n",
      "Snippet: In this class, we survey the wide space of inference-time techniques with a particular focus on the implementation and practical use cases of such methods.\n",
      "\n",
      "Title: CMU LLM Inference (1): Introduction to Language Models and ...\n",
      "URL: https://www.youtube.com/watch?v=F-mduXzNcRQ\n",
      "Snippet: This lecture (by Graham Neubig) for CMU CS 11-763, Advanced NLP (Fall 2025) covers: What is a language model? What is an inference algorithm ...\n"
     ]
    }
   ],
   "source": [
    "output = await search_tool(\"<query>Advisors offering Inference Algorithms for Language Modeling classes</query>\")\n",
    "print(search_tool.format_result(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc67b235",
   "metadata": {},
   "source": [
    "### A basic ReAct agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e490013f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§î Starting ReAct workflow for: Who are the staff members for CMU 11-763 Inference Algorithms for Language Modeling classes?\n",
      "üìä Will perform 3 think-search cycles\n",
      "================================================================================\n",
      "\n",
      "üîÑ CYCLE 1/3\n",
      "--------------------------------------------------\n",
      "üí≠ THINKING...\n",
      "üí° Reasoning: <think>From the conversation history, it appears that there is no previous information provided about the staff members for the CMU 11-763 Inference Algorithms for Language Modeling class. Thus, we ar...\n",
      "üîç GENERATING SEARCH...\n",
      "\n",
      "üîÑ CYCLE 2/3\n",
      "--------------------------------------------------\n",
      "üí≠ THINKING...\n",
      "üí° Reasoning: \n",
      "üîç GENERATING SEARCH...\n",
      "\n",
      "üîÑ CYCLE 3/3\n",
      "--------------------------------------------------\n",
      "üí≠ THINKING...\n",
      "üí° Reasoning: \n",
      "üîç GENERATING SEARCH...\n",
      "\n",
      "‚úÖ GENERATING FINAL ANSWER...\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from react_agent import *\n",
    "\n",
    "default_config_path = \"./react_agent.yaml\"\n",
    "\n",
    "workflow = ReActWorkflow(configuration=default_config_path)\n",
    "\n",
    "# print the config\n",
    "\n",
    "output = await workflow(\n",
    "    # question=\"Who are the target audience for CMU 11-763 Inference Algorithms for Language Modeling classes?\",\n",
    "    question=\"Who are the staff members for CMU 11-763 Inference Algorithms for Language Modeling classes?\",\n",
    "    max_tokens=2048,\n",
    "    temperature=0.7,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "final_answer, results, conversation_history, searched_queries = output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21426178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model answer:\n",
      "The staff members for the CMU 11-763 Inference Algorithms for Language Modeling class in Fall 2025 are as follows:\n",
      "\n",
      "1. **Instructors:**\n",
      "   - **Graham Neubig**: An associate professor at the Language Technologies Institute of Carnegie Mellon University, Graham Neubig is involved in teaching the course.\n",
      "   - **Amanda Bertsch**: A PhD student at the Language Technologies Institute at Carnegie Mellon University, advised by Matt Gormley and Graham Neubig, Amanda\n",
      "----------------------\n",
      "Tool calls:\n",
      "<query>CMU 11-763 Inference Algorithms for Language Modeling course staff members 2023</query><snippet id=6f8d1b72>\n",
      "Title: 11-664/763: Inference Algorithms for Language Modeling\n",
      "URL: https://www.phontron.com/class/lminference-fall2025/\n",
      "Snippet: In this class, we survey the wide space of inference-time techniques with a particular focus on the implementation and practical use cases of such methods.\n",
      "\n",
      "Title: CMU LLM Inference (1): Introduction to Language Models and ...\n",
      "URL: https://www.youtube.com/watch?v=F-mduXzNcRQ\n",
      "Snippet: This lecture (by Graham Neubig) for CMU CS 11-763, Advanced NLP (Fall 2025) covers: What is a language model? What is an inference algorithm ...\n",
      "</snippet><query>CMU 11-763 Inference Algorithms for Language Modeling course staff Fall 2025</query><snippet id=0c95d829>\n",
      "Title: 11-664/763: Inference Algorithms for Language Modeling\n",
      "URL: https://www.phontron.com/class/lminference-fall2025/\n",
      "Snippet: Fall 2025 @ Carnegie Mellon University. Graham Neubig ¬∑ gneubig@cs.cmu.edu ¬∑ Amanda Bertsch ¬∑ abertsch@cs.cmu.edu. Course Description. As the use of massive and ...\n",
      "\n",
      "Title: CMU Inference Algorithms for Language Modeling (Fall 2025)\n",
      "URL: https://www.youtube.com/playlist?list=PL8PYTP1V4I8DY15Ob83nmwBAVyNtUQadX\n",
      "Snippet: CMU Inference Algorithms for Language Modeling (Fall 2025) ¬∑ CMU LLM Inference (12): Reward Models and Best-of-N ¬∑ CMU LLM Inference (11): Agents and Multi-Agent ...\n",
      "</snippet><query>CMU 11-763 Inference Algorithms for Language Modeling course instructor and teaching assistants Fall 2025</query><snippet id=f1fcf617>\n",
      "Title: 11-664/763: Inference Algorithms for Language Modeling\n",
      "URL: https://www.phontron.com/class/lminference-fall2025/\n",
      "Snippet: In this class, we survey the wide space of inference-time techniques with a particular focus on the implementation and practical use cases of such methods.\n",
      "\n",
      "Title: CMU Inference Algorithms for Language Modeling (Fall 2025)\n",
      "URL: https://www.youtube.com/playlist?list=PL8PYTP1V4I8DY15Ob83nmwBAVyNtUQadX\n",
      "Snippet: CMU Inference Algorithms for Language Modeling (Fall 2025) ¬∑ CMU LLM Inference (12): Reward Models and Best-of-N ¬∑ CMU LLM Inference (11): Agents and Multi-Agent ...\n",
      "</snippet><query>CMU 11-763 Inference Algorithms for Language Modeling Graham Neubig and Amanda Bertsch roles Fall 2025</query><snippet id=7648f7c5>\n",
      "Title: 11-664/763: Inference Algorithms for Language Modeling\n",
      "URL: https://www.phontron.com/class/lminference-fall2025/\n",
      "Snippet: Inference-time algorithms can be applied on top of an already-trained model to improve generation quality, lower latency, or induce additional controllability.\n",
      "\n",
      "Title: Learn Inference Algorithms for Language Modeling at CMU - LinkedIn\n",
      "URL: https://www.linkedin.com/posts/kelvinmeeks_11-663763-lm-inference-activity-7341239778308214786-5NXw\n",
      "Snippet: Noteworthy: 11-663/763: Inference Algorithms for Language Modeling Fall 2025 Carnegie Mellon University Taught by Graham Neubig, and Amanda ...\n",
      "</snippet><query>Graham Neubig and Amanda Bertsch roles in CMU 11-763 Inference Algorithms for Language Modeling Fall 2025</query><snippet id=e89221f8>\n",
      "Title: 11-664/763: Inference Algorithms for Language Modeling\n",
      "URL: https://www.phontron.com/class/lminference-fall2025/\n",
      "Snippet: 11-664/763: Inference Algorithms for Language Modeling. Fall 2025 @ Carnegie Mellon University. Graham Neubig ¬∑ gneubig@cs.cmu.edu ¬∑ Amanda Bertsch ¬∑ abertsch@ ...\n",
      "\n",
      "Title: Graham Neubig - Language Technologies Institute\n",
      "URL: https://www.lti.cs.cmu.edu/people/faculty/neubig-graham.html\n",
      "Snippet: Graham Neubig is an associate professor at the Language Technologies Institute of Carnegie Mellon University. His research focuses on natural language ...\n",
      "</snippet><query>Amanda Bertsch role in CMU 11-763 Inference Algorithms for Language Modeling Fall 2025</query><snippet id=31fa22c8>\n",
      "Title: 11-664/763: Inference Algorithms for Language Modeling\n",
      "URL: https://www.phontron.com/class/lminference-fall2025/\n",
      "Snippet: In this class, we survey the wide space of inference-time techniques with a particular focus on the implementation and practical use cases of such methods.\n",
      "\n",
      "Title: CMU LLM Inference (4): Beam Search and Variants - YouTube\n",
      "URL: https://www.youtube.com/watch?v=2hhyfPYGCmY\n",
      "Snippet: This lecture (by Amanda Bertsch) for CMU CS 11-763, Advanced NLP (Fall 2025) covers: Beam search and variants Inadequacies of the mode Class ...\n",
      "</snippet><query>Amanda Bertsch teaching assistant or lecturer role in CMU 11-763 Fall 2025</query><snippet id=1f24161b>\n",
      "Title: Amanda Bertsch - CMU School of Computer Science\n",
      "URL: https://www.cs.cmu.edu/~abertsch/\n",
      "Snippet: I am a PhD student in the Language Technologies Institute at Carnegie Mellon University, advised by Matt Gormley and Graham Neubig.\n",
      "\n",
      "Title: Amanda Bertsch - Grad student at Carnegie Mellon ... - LinkedIn\n",
      "URL: https://www.linkedin.com/in/amandabertsch\n",
      "Snippet: I'm a PhD student in the Language Technologies Institute at Carnegie Mellon University, advised by Matt Gormley and Graham Neubig.\n",
      "</snippet><query>Amanda Bertsch teaching responsibilities in CMU 11-763 Inference Algorithms for Language Modeling Fall 2025</query><snippet id=e83ad801>\n",
      "Title: 11-664/763: Inference Algorithms for Language Modeling\n",
      "URL: https://www.phontron.com/class/lminference-fall2025/\n",
      "Snippet: In this class, we survey the wide space of inference-time techniques with a particular focus on the implementation and practical use cases of such methods.\n",
      "\n",
      "Title: Learn Inference Algorithms for Language Modeling at CMU - LinkedIn\n",
      "URL: https://www.linkedin.com/posts/kelvinmeeks_11-663763-lm-inference-activity-7341239778308214786-5NXw\n",
      "Snippet: Noteworthy: 11-663/763: Inference Algorithms for Language Modeling Fall 2025 Carnegie Mellon University Taught by Graham Neubig, and Amanda ...\n",
      "</snippet><query>Graham Neubig and Amanda Bertsch roles in teaching CMU 11-763 Fall 2025</query><snippet id=d6387396>\n",
      "Title: Graham Neubig - Language Technologies Institute\n",
      "URL: https://www.lti.cs.cmu.edu/people/faculty/neubig-graham.html\n",
      "Snippet: Graham Neubig is an associate professor at the Language Technologies Institute of Carnegie Mellon University. His research focuses on natural language ...\n",
      "\n",
      "Title: Staff | 11-664/763 LM Inference - Graham Neubig\n",
      "URL: https://www.phontron.com/class/lminference-fall2025/staff/\n",
      "Snippet: Instructors and Teaching Assistant for Fall 2025. Instructors. Graham Neubig ¬∑ gneubig@cs.cmu.edu ¬∑ Amanda Bertsch ¬∑ abertsch@cs.cmu.edu. Teaching Assistants ...\n",
      "</snippet>\n"
     ]
    }
   ],
   "source": [
    "final_answer, results, conversation_history, searched_queries = output\n",
    "print(\"Model answer:\")\n",
    "print(final_answer)\n",
    "print(\"----------------------\")\n",
    "print(\"Tool calls:\")\n",
    "print(dict(results)[\"tool_calls\"][1][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7713ce",
   "metadata": {},
   "source": [
    "### Your Task: the pipeline for Short-form Tasks\n",
    "\n",
    "You will work on applying the agent you just built to the graph and MMLU problems you explored in HW1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7e5b54d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Correct Prediction ===\n",
      "\n",
      "dict_keys(['score', 'matches', 'expected', 'predicted_count', 'correct_paths_found', 'incorrect_paths', 'missing_paths', 'message'])\n"
     ]
    }
   ],
   "source": [
    "# Build a simple agent for the graph problem\n",
    "from graph.graph_path_finder import *\n",
    "# YOUR_TASK_2.1, fix the GraphPathEvaluationTool (3 tasks, 6 lines of code)\n",
    "from mcp_agents.tool_interface.mcp_tools import GraphPathEvaluationTool\n",
    "\n",
    "correct_paths = [\n",
    "    {\"path\": [0, 1, 3, 7], \"weight\": 25},\n",
    "    {\"path\": [0, 2, 5, 7], \"weight\": 30}\n",
    "]\n",
    "\n",
    "eval_tool = GraphPathEvaluationTool(\n",
    "    correct_paths=correct_paths,\n",
    "    expected_count=2,\n",
    "    tool_start_tag=\"<predicted_paths>\",\n",
    "    tool_end_tag=\"</predicted_paths>\",\n",
    "    result_start_tag=\"<evaluation>\",\n",
    "    result_end_tag=\"</evaluation>\",\n",
    "    timeout=30\n",
    ")\n",
    "\n",
    "print(\"=== Correct Prediction ===\\n\")\n",
    "\n",
    "input1 = \"\"\"<predicted_paths>\n",
    "{\n",
    "    \"paths\": [[0, 1, 3, 7], [0, 2, 5, 7]],\n",
    "    \"weights\": [25, 30]\n",
    "}\n",
    "</predicted_paths>\"\"\"\n",
    "\n",
    "output1 = await eval_tool(input1)\n",
    "print(output1.keys())\n",
    "# print(json.dumps(output1, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "885bd052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Graph Path Finding Example ===\n",
      "\n",
      "1. Creating a random graph...\n",
      "Graph parameters: N=5, M=2, W=50, P=1\n",
      "Edges:\n",
      "  0 -> 1 (weight: 49)\n",
      "  0 -> 2 (weight: 5)\n",
      "  1 -> 2 (weight: 43)\n",
      "  1 -> 3 (weight: 36)\n",
      "  2 -> 3 (weight: 10)\n",
      "  2 -> 4 (weight: 25)\n",
      "  3 -> 4 (weight: 29)\n",
      "  3 -> 0 (weight: 20)\n",
      "  4 -> 0 (weight: 4)\n",
      "  4 -> 1 (weight: 46)\n",
      "\n",
      "2. Finding shortest path with dynamic programming...\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Graph Path Finding Example ===\\n\")\n",
    "\n",
    "# Create a simple example graph\n",
    "print(\"1. Creating a random graph...\")\n",
    "edges, params = create_random_graph(N=5, M=2, W=50, P=1)\n",
    "\n",
    "print(f\"Graph parameters: N={params['N']}, M={params['M']}, W={params['W']}, P={params['P']}\")\n",
    "print(\"Edges:\")\n",
    "for src, dst, weight in edges:\n",
    "    print(f\"  {src} -> {dst} (weight: {weight})\")\n",
    "\n",
    "# Find the correct solution\n",
    "print(\"\\n2. Finding shortest path with dynamic programming...\")\n",
    "solution = find_top_p_paths(edges, params[\"N\"], params[\"P\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0465c4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = generate_problem_prompt(edges, params[\"N\"], params[\"P\"])\n",
    "\n",
    "llm_response = query_llm_with_function_call(prompt, \"gpt-4o\", os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "predicted_solution = convert_llm_response_to_solution(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "315e890f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Model Prediction ===\n",
      "\n",
      "dict_keys(['score', 'matches', 'expected', 'predicted_count', 'correct_paths_found', 'incorrect_paths', 'missing_paths', 'message'])\n",
      "{\n",
      "  \"score\": 0.0,\n",
      "  \"matches\": 0,\n",
      "  \"expected\": 1,\n",
      "  \"predicted_count\": 1,\n",
      "  \"correct_paths_found\": [],\n",
      "  \"incorrect_paths\": [\n",
      "    {\n",
      "      \"path\": [\n",
      "        0,\n",
      "        2,\n",
      "        3,\n",
      "        4\n",
      "      ],\n",
      "      \"weight\": 44\n",
      "    }\n",
      "  ],\n",
      "  \"missing_paths\": [\n",
      "    {\n",
      "      \"path\": [\n",
      "        0,\n",
      "        2,\n",
      "        4\n",
      "      ],\n",
      "      \"weight\": 30\n",
      "    }\n",
      "  ],\n",
      "  \"message\": \"Found 0/1 correct paths (0.0%)\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "def solution_to_dict_list(solution: GraphPathSolution) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Convert GraphPathSolution to list of dict format.\n",
    "    \n",
    "    Args:\n",
    "        solution: GraphPathSolution object\n",
    "    \n",
    "    Returns:\n",
    "        List of dicts with 'path' and 'weight' keys\n",
    "    \"\"\"\n",
    "    return [\n",
    "        {\"path\": path_info.path, \"weight\": path_info.weight}\n",
    "        for path_info in solution.paths\n",
    "    ]\n",
    "\n",
    "\n",
    "eval_tool = GraphPathEvaluationTool(\n",
    "    correct_paths=solution_to_dict_list(solution),\n",
    "    expected_count=len(solution_to_dict_list(solution)),\n",
    "    tool_start_tag=\"<predicted_paths>\",\n",
    "    tool_end_tag=\"</predicted_paths>\",\n",
    "    result_start_tag=\"<evaluation>\",\n",
    "    result_end_tag=\"</evaluation>\",\n",
    "    timeout=30\n",
    ")\n",
    "\n",
    "print(\"=== Model Prediction ===\\n\")\n",
    "\n",
    "input2 = f\"\"\"<predicted_paths>\n",
    "{json.dumps(solution_to_dict_list(predicted_solution), indent=2)}\n",
    "</predicted_paths>\"\"\"\n",
    "\n",
    "output2 = await eval_tool(input2)\n",
    "print(output2.keys())\n",
    "print(json.dumps(output2, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f62e0b6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "275fff77b22c4eb4b38f4e9bd5e5abd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07009a2a9c5d4a99b6d867da6b46bdf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "dataset_infos.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67e11abae4f04a19ab2b1384cbe84eec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "college_medicine/test-00000-of-00001.par(‚Ä¶):   0%|          | 0.00/42.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae9ff344fc4b4a2998a9469097c7b4da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "college_medicine/validation-00000-of-000(‚Ä¶):   0%|          | 0.00/8.99k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16c1d6a6a05849d0a65966a105c39fcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "college_medicine/dev-00000-of-00001.parq(‚Ä¶):   0%|          | 0.00/4.84k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfa298f9bd6e47fb9d57d2cb8a6868a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/173 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f150d4db9314457a8514a8d895d504f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/22 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "322f90685ecf4c4e81a6fc443ef709b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating dev split:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded: 173 examples\n"
     ]
    }
   ],
   "source": [
    "# Build the inference pipeline for MMLU\n",
    "from inference.inference import load_custom_dataset, convert_llm_response_to_solution, format_example, format_subject\n",
    "\n",
    "examples = load_custom_dataset(\"MMLU-preview\")\n",
    "\n",
    "print(f\"Dataset loaded: {len(examples)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5912f972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating responses for 30 MMLU examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating examples:  23%|‚ñà‚ñà‚ñé       | 7/30 [09:33<31:53, 83.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st answer extract failed\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating examples:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 25/30 [40:33<08:20, 100.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: MCP call timed out: Timed out while waiting for response to ClientRequest. Waited 60.0 seconds.\n",
      "Error: MCP call timed out: Timed out while waiting for response to ClientRequest. Waited 60.0 seconds.\n",
      "Error: MCP call timed out: Timed out while waiting for response to ClientRequest. Waited 60.0 seconds.\n",
      "Error: MCP call timed out: Timed out while waiting for response to ClientRequest. Waited 60.0 seconds.\n",
      "Error: MCP call timed out: Timed out while waiting for response to ClientRequest. Waited 60.0 seconds.\n",
      "Error: MCP call timed out: Timed out while waiting for response to ClientRequest. Waited 60.0 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating examples:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 29/30 [53:09<02:18, 138.54s/it]Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x7d16f75c0620>\n",
      "Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x7d16df6b2f60>\n",
      "Evaluating examples: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [54:34<00:00, 109.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average score: 0.87\n",
      "Generated 30 responses\n",
      "Sample response:\n",
      "The scenario presented highlights the influence of unconscious bias in the police officer's behavior, particularly how his past experiences, specifically childhood trauma, shape his current actions. The fact that he is more likely to ticket middle-aged white males with dark hair and eyes‚Äîwho resemble his father‚Äîsuggests a deeper psychological association that he is not consciously aware of.\n",
      "\n",
      "To understand this unconscious bias, the psychoanalytic framework is most relevant. This framework, rooted in the ideas of Sigmund Freud and further developed by others, emphasizes the role of unconscious processes and early childhood experiences in influencing behavior. In this case, the officer's abusive relationship with his father likely instilled unconscious associations that affect his perceptions and actions towards individuals who resemble that figure from his past.\n",
      "\n",
      "While other frameworks, such as cognitive behavioral approaches, focus on the thoughts and behaviors that can be consciously changed, they do not specifically address the deep-seated unconscious motivations that psychoanalysis delves into. Humanistic and behaviorist frameworks also do not adequately account for the unconscious influences stemming from early childhood experiences.\n",
      "\n",
      "Thus, the most appropriate psychological framework that would directly address the unconscious bias in the officer's behavior is:\n",
      "\n",
      "The answer is B. Psychoanalytic.\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Separate generation and evaluation functions\n",
    "async def udated_mmlu_pipeline(examples, default_config_path):\n",
    "    \"\"\"Generate responses for all examples without evaluation\"\"\"\n",
    "    \n",
    "    print(\"Generating responses for\", len(examples), \"MMLU examples\")\n",
    "\n",
    "    workflow = ReActWorkflow(configuration=default_config_path)\n",
    "    base_agent_prompt = workflow.answer_agent.prompt\n",
    "\n",
    "    choices = [\"A\", \"B\", \"C\", \"D\"]\n",
    "\n",
    "\n",
    "    results = []\n",
    "    total_score = 0.0\n",
    "\n",
    "    for i, example in tqdm(enumerate(examples, 1), total=len(examples), desc=\"Evaluating examples\"):\n",
    "        question = example[\"question\"] # format_example(example, include_answer=False)\n",
    "        correct_answer = choices[example[\"answer\"]]\n",
    "\n",
    "        # YOUR_TASK_2.2: what is the additional instructions here?\n",
    "        # Hint: check the MMLU inference pipeline to understand what to specify; 1 line of code\n",
    "        additional_instructions = f\"The following is a multiple choice question (with answers) about {format_subject(example['subject'])}. Output the answer in the format of \\\"The answer is (X)\\\" at the end.\\n\\n\"\n",
    "\n",
    "        workflow.answer_agent.prompt = base_agent_prompt + \"\\n\" + additional_instructions + format_example(example, include_answer=False)\n",
    "\n",
    "        output = await workflow(\n",
    "            question=question,\n",
    "            max_tokens=4096,\n",
    "            temperature=0.7,\n",
    "            verbose=False,\n",
    "        )\n",
    "\n",
    "        final_answer, answer_result, conversation_history, searched_queries = output\n",
    "\n",
    "        predicted_solution = convert_llm_response_to_solution(final_answer, \"MMLU\")\n",
    "\n",
    "        score = (choices[example[\"answer\"]] == predicted_solution)\n",
    "        total_score += score\n",
    "\n",
    "        results.append({\n",
    "            \"example_id\": i,    \n",
    "            \"question\": question,\n",
    "            \"correct_answer\": correct_answer,\n",
    "            \"predicted_solution\": predicted_solution,\n",
    "            \"final_answer\": final_answer,\n",
    "            \"generation\": answer_result.model_dump(),\n",
    "            \"conversation_history\": conversation_history,\n",
    "            \"searched_queries\": searched_queries,\n",
    "        })\n",
    "\n",
    "    average_score = total_score / len(examples) if examples else 0.0\n",
    "\n",
    "    print(f\"Average score: {average_score:.2f}\")\n",
    "    \n",
    "    output_config = {k:v for k,v in dict(workflow.configuration).items() if \"api_key\" not in k}\n",
    "\n",
    "    return {\n",
    "        \"config\": output_config,\n",
    "        \"average_score\": average_score,\n",
    "        \"total_examples\": len(examples),\n",
    "        \"results\": results\n",
    "    }\n",
    "\n",
    "\n",
    "default_config_path = \"./react_agent_mmlu.yaml\"\n",
    "\n",
    "# YOUR_TASK_2.2: Run the inference for 30 examples after fixing the function:\n",
    "# save the answers using the code at the next block\n",
    "# report the acc. at the home write-up\n",
    "output = await udated_mmlu_pipeline(examples[:30], default_config_path)\n",
    "\n",
    "# Inspect or save generated responses here if needed\n",
    "print(f\"Generated {len(output['results'])} responses\")\n",
    "print(\"Sample response:\")\n",
    "print(output[\"results\"][0][\"final_answer\"])\n",
    "print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "86191ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-4o-mini\n",
      "0.8666666666666667\n",
      "## Question: A police officer carries out hundreds of traffic stops every year. When his supervisor is reviewing the officer‚Äôs records for the past year, he notices that the officer is equally likely to stop people of various genders, ages, and races. However, he is significantly more likely to write tickets for middle-aged white males with dark hair and eyes. When confronted with this fact, the officer truthfully states that he has no idea why that is, and that it must simply be a coincidence. Unbeknownst to the officer, this behavior is tied to the fact that these men look like his father, with whom he had an abusive relationship as a child. What psychological framework would directly address the unconscious bias in his behavior? \n",
      "## Correct Answer: B\n",
      "## Final Answer:\n",
      "The scenario presented highlights the influence of unconscious bias in the police officer's behavior, particularly how his past experiences, specifically childhood trauma, shape his current actions. The fact that he is more likely to ticket middle-aged white males with dark hair and eyes‚Äîwho resemble his father‚Äîsuggests a deeper psychological association that he is not consciously aware of.\n",
      "\n",
      "To understand this unconscious bias, the psychoanalytic framework is most relevant. This framework, rooted in the ideas of Sigmund Freud and further developed by others, emphasizes the role of unconscious processes and early childhood experiences in influencing behavior. In this case, the officer's abusive relationship with his father likely instilled unconscious associations that affect his perceptions and actions towards individuals who resemble that figure from his past.\n",
      "\n",
      "While other frameworks, such as cognitive behavioral approaches, focus on the thoughts and behaviors that can be consciously changed, they do not specifically address the deep-seated unconscious motivations that psychoanalysis delves into. Humanistic and behaviorist frameworks also do not adequately account for the unconscious influences stemming from early childhood experiences.\n",
      "\n",
      "Thus, the most appropriate psychological framework that would directly address the unconscious bias in the officer's behavior is:\n",
      "\n",
      "The answer is B. Psychoanalytic.\n",
      "## Searched Queries: ['psychological frameworks addressing unconscious bias in law enforcement implicit bias childhood trauma impact training programs', '']\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "## Question: Who set the world record for the mile race in 1886?\n",
      "## Correct Answer: D\n",
      "## Final Answer:\n",
      "The world record for the mile race in 1886 was set by Walter George on August 23, during a highly publicized race at Lillie Bridge in London. He completed the mile in a remarkable time of 4:12.8, which was reported as 4:12¬æ at the time. This achievement was particularly significant as it marked the first time a human had completed the mile in under 4 minutes and 13 seconds, and the record stood unbroken for nearly 30 years, showcasing its historical importance in the realm of athletics.\n",
      "\n",
      "The race attracted a large crowd of approximately 20,000 spectators, indicating the level of excitement and interest in the sport at that time. Walter George's primary competitor in this race was William Cummings, with whom he had a notable rivalry. George's victory in this match solidified his reputation as one of the premier middle-distance runners of his era.\n",
      "\n",
      "This record was officially recognized, contributing to George's status in the annals of athletic history as a significant figure in the development of competitive running. His record not only exemplified his exceptional talent but also inspired future generations of athletes.\n",
      "\n",
      "The answer is D.\n",
      "## Searched Queries: ['world record mile race 1886 athlete name time details', '']\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = output[\"config\"][\"react_agent_model_name\"]\n",
    "model_display = model.split(\"/\")[-1]\n",
    "len_examples = output[\"total_examples\"]\n",
    "display_config = default_config_path.split(\"/\")[-1].replace(\".yaml\", \"\")\n",
    "print(model_display)\n",
    "\n",
    "with open(f\"results_{model_display}_{len_examples}_{display_config}.json\", \"w\") as f:\n",
    "    json.dump(output, f, indent=2)\n",
    "\n",
    "print(output[\"average_score\"])\n",
    "\n",
    "for response in output[\"results\"][:2]:\n",
    "    print(\"## Question: \" + response[\"question\"])\n",
    "    print(\"## Correct Answer: \" + response[\"correct_answer\"])\n",
    "    print(\"## Final Answer:\\n\" + response[\"final_answer\"])\n",
    "    print(\"## Searched Queries: \" + str(response[\"searched_queries\"]))\n",
    "    print(\"----\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c4ccd7",
   "metadata": {},
   "source": [
    "### Simple analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3b3adc33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 80.2,\n",
       " 'thinking': 693.47,\n",
       " 'query': 24.1,\n",
       " 'snippets_titles': 7.0,\n",
       " 'snippets_snippets': 7.0,\n",
       " 'final_answer': 317.53}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_thoughts(text: str) -> str:\n",
    "    match = re.search(r\"<think>(.*?)</think>\", text, re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    return text\n",
    "\n",
    "def extract_search_query(text: str) -> str:\n",
    "    match = re.search(r\"<query>(.*?)</query>\", text, re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    return text\n",
    "\n",
    "def parse_xml_snippets(text):\n",
    "\n",
    "    pattern = re.compile(\n",
    "        r\"<snippet id=([^>]+)>\"  # Capture Group 1: The ID\n",
    "        # YOUR_TASK_3.1: three lines of code here to process the retrieved snippets\n",
    "        r\"Title: (.+?)\\n\"         # YOUR_TASK_3.1: Capture Group 2: The Title\n",
    "        r\"(?:URL: (.+?)\\n)?\"      # Optional Group with Capture Group 3: The URL\n",
    "        r\"Snippet: (.+?)\\n\"       # Capture Group 4: The Snippet\n",
    "        r\"\\s*</snippet>\",          # The closing tag\n",
    "        re.DOTALL\n",
    "        )\n",
    "\n",
    "    matches = pattern.findall(text)\n",
    "\n",
    "    results = []\n",
    "    for match in matches:\n",
    "        # match is a tuple: (id, title, url, snippet)\n",
    "        # If the optional URL group did not match, match[2] will be None.\n",
    "        snippet_id = match[0].strip()\n",
    "        title = match[1].strip()\n",
    "        url = match[2].strip() if match[2] else \"\"  # Handle None case for missing URL\n",
    "        snippet = match[3].strip()\n",
    "\n",
    "        results.append({\n",
    "            \"Title\": title,\n",
    "            \"URL\": url,\n",
    "            \"Snippet\": snippet\n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def count_tokens(text, model=\"openai/gpt-4o\"):\n",
    "    \"\"\"\n",
    "    Counts the number of tokens in a prompt using LiteLLM's token counting utility.\n",
    "    Args:\n",
    "        prompt (str): The input prompt string.\n",
    "        model (str): The model name for which to count tokens (default: \"gpt-3.5-turbo\").\n",
    "    Returns:\n",
    "        int: The number of tokens in the prompt.\n",
    "    \"\"\"\n",
    "    return litellm.token_counter(model=model, messages=[{\"role\": \"user\", \"content\": text}])\n",
    "\n",
    "\n",
    "def count_tokens_in_results(results):\n",
    "    # report the numbers of tokens used for question,thinking, query, snippets, and final answer\n",
    "    report_results = []\n",
    "    for result in results:\n",
    "\n",
    "        thoughts = [one_round[\"content\"] for one_round in result[\"conversation_history\"] if one_round[\"type\"] == \"think\"]\n",
    "        cleaned_thoughts = [extract_thoughts(thought) for thought in thoughts]\n",
    "        \n",
    "        query_snippets = [one_round[\"content\"] for one_round in result[\"conversation_history\"] if one_round[\"type\"] == \"query\"]\n",
    "        cleaned_queries = [extract_search_query(query) for query in query_snippets]\n",
    "        # print(parse_xml_snippets(query_snippets[0]))\n",
    "        parsed_snippets = []\n",
    "        for query in query_snippets:\n",
    "            parsed_snippets.extend(parse_xml_snippets(query))\n",
    "        cleaned_snippets_titles = [snippet[\"Title\"] for snippet in parsed_snippets]\n",
    "        cleaned_snippets_snippets = [snippet[\"Snippet\"] for snippet in parsed_snippets]\n",
    "\n",
    "        report_results.append({\n",
    "            \"question\": count_tokens(result[\"question\"]),\n",
    "            \"thinking\": count_tokens(\" \".join(cleaned_thoughts)),\n",
    "            \"query\": count_tokens(\" \".join(cleaned_queries)),   \n",
    "            \"snippets_titles\": count_tokens(\" \".join(cleaned_snippets_titles)),\n",
    "            \"snippets_snippets\": count_tokens(\" \".join(cleaned_snippets_snippets)),\n",
    "            \"final_answer\": count_tokens(result[\"final_answer\"]),\n",
    "        })\n",
    "\n",
    "    # for each key, report the average, round to 2 decimal places\n",
    "    return {key: round(sum(result[key] for result in report_results) / len(report_results), 2) for key in report_results[0].keys()}\n",
    "\n",
    "# YOUR_TASK_3.2: calculate the token counts for each category for each variant and report in the homework write-up\n",
    "count_tokens_in_results(output[\"results\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b52c36-7c84-4f07-9300-e2d8161efa5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
